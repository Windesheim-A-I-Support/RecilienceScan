import pandas as pd
import numpy as np
from pathlib import Path
import glob
import json
from datetime import datetime
import random

# Configuration
DATA_DIR = "./data"
CLEANED_CSV = "./data/cleaned_master.csv"
VALIDATION_OUTPUT = "./data/integrity_validation_report.json"
REPORT_OUTPUT = "./data/integrity_validation_report.txt"

# Excel file patterns (same as convert_data.py)
FILE_PATTERNS = [
    "Resilience - MasterDatabase*.xlsx",
    "Resilience - MasterDatabase*.xls",
    "MasterDatabase*.xlsx",
    "MasterDatabase*.xls",
    "*.xlsx",
    "*.xls",
]

class DataIntegrityValidator:
    def __init__(self):
        self.issues = []
        self.warnings = []
        self.info = []
        self.samples = []
        self.statistics = {
            'total_records_excel': 0,
            'total_records_csv': 0,
            'samples_validated': 0,
            'perfect_matches': 0,
            'acceptable_matches': 0,
            'mismatches': 0,
            'missing_in_csv': 0
        }

    def log(self, level, message, details=None):
        """Log a message with level"""
        entry = {
            'level': level,
            'message': message,
            'timestamp': datetime.now().isoformat()
        }
        if details:
            entry['details'] = details

        if level == 'ERROR':
            self.issues.append(entry)
        elif level == 'WARNING':
            self.warnings.append(entry)
        else:
            self.info.append(entry)

        symbol = '[X]' if level == 'ERROR' else '[!]' if level == 'WARNING' else '[i]'
        print(f"{symbol} [{level}] {message}")

    def find_excel_file(self):
        """Find the source Excel file"""
        print("\n" + "="*70)
        print("FINDING SOURCE EXCEL FILE")
        print("="*70)

        for pattern in FILE_PATTERNS:
            search_path = Path(DATA_DIR) / pattern
            matches = list(glob.glob(str(search_path)))

            if matches:
                matches.sort(key=lambda x: Path(x).stat().st_mtime, reverse=True)
                excel_file = matches[0]
                self.log('INFO', f"Found Excel file: {excel_file}")
                if len(matches) > 1:
                    self.log('INFO', f"Multiple files found, using most recent")
                return excel_file

        self.log('ERROR', "No Excel file found in data directory")
        return None

    def load_excel_data(self, excel_path):
        """Load data from Excel file"""
        print("\n" + "="*70)
        print("LOADING EXCEL DATA")
        print("="*70)

        try:
            # Load with no header to detect it ourselves
            df_raw = pd.read_excel(excel_path, header=None)
            self.log('INFO', f"Loaded raw Excel: {df_raw.shape[0]} rows x {df_raw.shape[1]} columns")

            # Detect header row (same logic as convert_data.py)
            header_keywords = ['company', 'name', 'email', 'submitdate', 'up -', 'in -', 'do -']
            header_row_idx = 0

            for idx in range(min(10, len(df_raw))):
                row_strings = [str(v).lower() if pd.notna(v) else '' for v in df_raw.iloc[idx]]
                keyword_matches = sum(
                    any(keyword in s for keyword in header_keywords)
                    for s in row_strings
                )
                if keyword_matches >= 3:
                    header_row_idx = idx
                    self.log('INFO', f"Detected header at row {idx + 1}")
                    break

            # Extract data with proper header
            header = df_raw.iloc[header_row_idx].tolist()
            data = df_raw.iloc[header_row_idx + 1:]
            df = pd.DataFrame(data.values, columns=header)

            # Clean column names (same as convert_data.py)
            import re
            cleaned_cols = []
            for col in df.columns:
                col_str = str(col).strip().lower()
                col_str = col_str.replace(' ', '_').replace('-', '').replace(':', '').replace('(', '').replace(')', '')
                col_str = re.sub(r'[^\w_]', '', col_str)
                cleaned_cols.append(col_str)
            df.columns = cleaned_cols

            # Remove empty rows
            df = df.dropna(how='all')

            self.statistics['total_records_excel'] = len(df)
            self.log('INFO', f"Excel data rows: {len(df)}")

            return df

        except Exception as e:
            self.log('ERROR', f"Failed to load Excel: {e}")
            return None

    def load_csv_data(self):
        """Load cleaned CSV data"""
        print("\n" + "="*70)
        print("LOADING CLEANED CSV")
        print("="*70)

        if not Path(CLEANED_CSV).exists():
            self.log('ERROR', f"Cleaned CSV not found: {CLEANED_CSV}")
            return None

        try:
            df = pd.read_csv(CLEANED_CSV)
            df.columns = df.columns.str.lower().str.strip()
            self.statistics['total_records_csv'] = len(df)
            self.log('INFO', f"CSV data rows: {len(df)}")
            return df

        except Exception as e:
            self.log('ERROR', f"Failed to load CSV: {e}")
            return None

    def create_record_key(self, row):
        """Create a unique key for a record"""
        company = str(row.get('company_name', '')).strip().lower()
        name = str(row.get('name', '')).strip().lower()
        email = str(row.get('email_address', '')).strip().lower()
        return f"{company}||{name}||{email}"

    def compare_score_values(self, excel_val, csv_val, tolerance=0.01):
        """Compare two score values with tolerance"""
        # Handle various representations of missing/invalid data
        excel_empty = pd.isna(excel_val) or str(excel_val).strip() in ['', '?', 'nan']
        csv_empty = pd.isna(csv_val) or str(csv_val).strip() in ['', '?', 'nan']

        if excel_empty and csv_empty:
            return 'match', 'both_empty'
        if excel_empty != csv_empty:
            return 'mismatch', 'one_empty'

        try:
            # Convert to float, handling European decimals
            excel_float = float(str(excel_val).replace(',', '.'))
            csv_float = float(csv_val)

            # Check if values match within tolerance
            if abs(excel_float - csv_float) <= tolerance:
                return 'match', 'values_equal'
            else:
                return 'mismatch', f'values_differ: {excel_float:.2f} vs {csv_float:.2f}'
        except:
            return 'mismatch', 'conversion_error'

    def validate_sample(self, excel_row, csv_row, sample_num):
        """Validate a single sampled record"""
        validation_result = {
            'sample_num': sample_num,
            'company': excel_row.get('company_name', 'Unknown'),
            'person': excel_row.get('name', 'Unknown'),
            'email': excel_row.get('email_address', 'Unknown'),
            'fields_checked': 0,
            'fields_matched': 0,
            'mismatches': [],
            'status': 'unknown'
        }

        # Check basic fields
        basic_fields = ['company_name', 'name', 'email_address']
        for field in basic_fields:
            excel_val = str(excel_row.get(field, '')).strip()
            csv_val = str(csv_row.get(field, '')).strip()
            validation_result['fields_checked'] += 1

            if excel_val.lower() == csv_val.lower():
                validation_result['fields_matched'] += 1
            else:
                validation_result['mismatches'].append({
                    'field': field,
                    'excel': excel_val,
                    'csv': csv_val
                })

        # Check score columns
        score_columns = ['up__r', 'up__c', 'up__f', 'up__v', 'up__a',
                        'in__r', 'in__c', 'in__f', 'in__v', 'in__a',
                        'do__r', 'do__c', 'do__f', 'do__v', 'do__a']

        for col in score_columns:
            if col in excel_row.index and col in csv_row.index:
                validation_result['fields_checked'] += 1
                status, detail = self.compare_score_values(excel_row[col], csv_row[col])

                if status == 'match':
                    validation_result['fields_matched'] += 1
                else:
                    validation_result['mismatches'].append({
                        'field': col,
                        'excel': str(excel_row[col]),
                        'csv': str(csv_row[col]),
                        'detail': detail
                    })

        # Determine overall status
        if validation_result['fields_checked'] == 0:
            validation_result['status'] = 'no_fields'
        elif validation_result['fields_matched'] == validation_result['fields_checked']:
            validation_result['status'] = 'perfect'
            self.statistics['perfect_matches'] += 1
        elif validation_result['fields_matched'] / validation_result['fields_checked'] >= 0.9:
            validation_result['status'] = 'acceptable'
            self.statistics['acceptable_matches'] += 1
        else:
            validation_result['status'] = 'mismatch'
            self.statistics['mismatches'] += 1

        return validation_result

    def validate_samples(self, excel_df, csv_df, num_samples=10):
        """Validate random samples from Excel against CSV"""
        print("\n" + "="*70)
        print(f"VALIDATING {num_samples} RANDOM SAMPLES")
        print("="*70)

        # Create lookup dictionary for CSV records
        csv_lookup = {}
        for idx, row in csv_df.iterrows():
            key = self.create_record_key(row)
            csv_lookup[key] = row

        self.log('INFO', f"Created CSV lookup with {len(csv_lookup)} unique records")

        # Sample random records from Excel
        if len(excel_df) < num_samples:
            num_samples = len(excel_df)
            self.log('WARNING', f"Excel has fewer than {num_samples} records, sampling all")

        sample_indices = random.sample(range(len(excel_df)), num_samples)

        for i, idx in enumerate(sample_indices, 1):
            excel_row = excel_df.iloc[idx]
            key = self.create_record_key(excel_row)

            print(f"\n[{i}/{num_samples}] Validating: {excel_row.get('company_name', 'Unknown')}")

            if key not in csv_lookup:
                self.log('WARNING', f"Record not found in CSV: {excel_row.get('company_name', 'Unknown')}")
                self.statistics['missing_in_csv'] += 1
                self.samples.append({
                    'sample_num': i,
                    'company': excel_row.get('company_name', 'Unknown'),
                    'status': 'missing_in_csv'
                })
                continue

            csv_row = csv_lookup[key]
            validation_result = self.validate_sample(excel_row, csv_row, i)
            self.samples.append(validation_result)
            self.statistics['samples_validated'] += 1

            # Log result
            if validation_result['status'] == 'perfect':
                self.log('INFO', f"Perfect match: {validation_result['fields_matched']}/{validation_result['fields_checked']} fields")
            elif validation_result['status'] == 'acceptable':
                self.log('INFO', f"Acceptable match: {validation_result['fields_matched']}/{validation_result['fields_checked']} fields")
            else:
                self.log('WARNING', f"Mismatches found: {len(validation_result['mismatches'])} issues")

    def generate_report(self):
        """Generate human-readable report"""
        lines = []
        lines.append("="*70)
        lines.append("DATA INTEGRITY VALIDATION REPORT")
        lines.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("="*70)

        lines.append("\nOVERVIEW:")
        lines.append(f"  Excel file records: {self.statistics['total_records_excel']}")
        lines.append(f"  CSV file records: {self.statistics['total_records_csv']}")
        lines.append(f"  Records removed during cleaning: {self.statistics['total_records_excel'] - self.statistics['total_records_csv']}")

        lines.append("\nSAMPLE VALIDATION RESULTS:")
        lines.append(f"  Samples validated: {self.statistics['samples_validated']}")
        lines.append(f"  Perfect matches: {self.statistics['perfect_matches']}")
        lines.append(f"  Acceptable matches (>90%): {self.statistics['acceptable_matches']}")
        lines.append(f"  Mismatches: {self.statistics['mismatches']}")
        lines.append(f"  Missing in CSV: {self.statistics['missing_in_csv']}")

        accuracy = 0
        if self.statistics['samples_validated'] > 0:
            accuracy = ((self.statistics['perfect_matches'] + self.statistics['acceptable_matches']) /
                       self.statistics['samples_validated'] * 100)
            lines.append(f"\n  Overall accuracy: {accuracy:.1f}%")

        lines.append("\nDETAILED SAMPLE RESULTS:")
        for sample in self.samples:
            if sample.get('status') == 'missing_in_csv':
                lines.append(f"\n  [{sample['sample_num']}] {sample['company']}")
                lines.append(f"      Status: MISSING IN CSV")
            else:
                lines.append(f"\n  [{sample['sample_num']}] {sample['company']} - {sample['person']}")
                lines.append(f"      Status: {sample['status'].upper()}")
                lines.append(f"      Matched: {sample['fields_matched']}/{sample['fields_checked']} fields")

                if sample['mismatches']:
                    lines.append(f"      Mismatches:")
                    for mm in sample['mismatches'][:5]:  # Limit to first 5
                        lines.append(f"        - {mm['field']}: Excel='{mm['excel']}' vs CSV='{mm['csv']}'")
                    if len(sample['mismatches']) > 5:
                        lines.append(f"        ... and {len(sample['mismatches']) - 5} more")

        lines.append("\n" + "="*70)
        lines.append("VERDICT:")
        if self.statistics['samples_validated'] == 0:
            lines.append("  [!] No samples could be validated")
            lines.append("  [!] All sampled records were missing in CSV")
            lines.append("  [!] This suggests records were removed during cleaning")
        elif self.statistics['mismatches'] == 0 and self.statistics['missing_in_csv'] == 0:
            lines.append("  [OK] Data integrity verified!")
            lines.append("  [OK] Cleaning process preserves data accurately")
        elif accuracy >= 90:
            lines.append("  [OK] Data integrity is acceptable (>90% accuracy)")
            lines.append("  [!] Minor discrepancies found - review details above")
        else:
            lines.append("  [!] Data integrity issues detected")
            lines.append("  [!] Significant discrepancies between Excel and CSV")
            lines.append("  [!] Review cleaning process")

        lines.append("="*70)

        report_text = '\n'.join(lines)
        print("\n" + report_text)

        with open(REPORT_OUTPUT, 'w') as f:
            f.write(report_text)

        self.log('INFO', f"Report saved: {REPORT_OUTPUT}")

    def save_validation_log(self):
        """Save detailed JSON log"""
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'statistics': self.statistics,
            'samples': self.samples,
            'errors': self.issues,
            'warnings': self.warnings,
            'info': self.info
        }

        with open(VALIDATION_OUTPUT, 'w') as f:
            json.dump(log_data, f, indent=2)

        self.log('INFO', f"Validation log saved: {VALIDATION_OUTPUT}")

def main(num_samples=10):
    """Main validation function"""
    print("="*70)
    print("DATA INTEGRITY VALIDATOR")
    print("Validates that cleaning process preserves data accurately")
    print("="*70)

    validator = DataIntegrityValidator()

    # Find and load Excel file
    excel_file = validator.find_excel_file()
    if not excel_file:
        print("\n[FAILED] Cannot proceed without Excel file")
        return False

    excel_df = validator.load_excel_data(excel_file)
    if excel_df is None:
        print("\n[FAILED] Cannot load Excel data")
        return False

    # Load CSV file
    csv_df = validator.load_csv_data()
    if csv_df is None:
        print("\n[FAILED] Cannot load CSV data")
        return False

    # Validate samples
    validator.validate_samples(excel_df, csv_df, num_samples)

    # Generate reports
    validator.generate_report()
    validator.save_validation_log()

    print("\n" + "="*70)
    print("[COMPLETE] Data integrity validation finished")
    print("="*70)
    print(f"[REPORT] Text report: {REPORT_OUTPUT}")
    print(f"[LOG] JSON log: {VALIDATION_OUTPUT}")
    print("="*70)

    return True

if __name__ == "__main__":
    import sys

    # Allow specifying number of samples via command line
    num_samples = 10
    if len(sys.argv) > 1:
        try:
            num_samples = int(sys.argv[1])
        except:
            print("Usage: python validate_data_integrity.py [num_samples]")
            sys.exit(1)

    success = main(num_samples)
    exit(0 if success else 1)
