# Manual Testing Guide

## Overview
This guide provides step-by-step instructions for manually testing all new features to ensure nothing broke and all features work as expected.

**Automated Validation Status:** ‚úÖ All 12 automated tests passed
- Data quality dashboard script works
- Data cleaner script works
- All parameters configured correctly
- GUI checkboxes and buttons present
- Email priority fallback configured

**Now test these features manually in the GUI:**

---

## Test 1: Existing Functionality (Baseline Test)

**Purpose:** Verify nothing broke in existing features

### Steps:
1. Launch GUI: `python ResilienceScanGUI.py`
2. Go to **Data** tab
3. Click **Load Data** - verify 467 records load
4. Go to **Reports** tab
5. Select company: **Abbott**
6. Select person: **Lisa Daly**
7. **DO NOT** check Debug Mode or Demo Mode
8. Click **Generate Single Report**

### Expected Results:
- ‚úÖ Report generates successfully without errors
- ‚úÖ PDF opens automatically
- ‚úÖ Report shows correct company name: Abbott
- ‚úÖ Report shows correct person name: Lisa Daly
- ‚úÖ Report does NOT have debug table at end
- ‚úÖ Scores match data from CSV

### Validation Check:
Open `data/cleaned_master.csv` and verify Abbott - Lisa Daly scores match report.

---

## Test 2: Debug Mode Feature

**Purpose:** Verify debug mode adds raw data table to end of report

### Steps:
1. In GUI, still on **Reports** tab
2. Company: **Abbott**
3. Person: **Lisa Daly**
4. ‚úÖ **CHECK** "Debug Mode (show raw data table at end of report)"
5. ‚ùå **UNCHECK** Demo Mode
6. Click **Generate Single Report**

### Expected Results:
- ‚úÖ Report generates successfully
- ‚úÖ Report has a new page at the very end
- ‚úÖ New page titled: "Debug: Raw Score Values"
- ‚úÖ Table shows:
  - Company: Abbott | Person: Lisa Daly
  - All 15 raw scores (up__r, up__c, up__f, up__v, up__a, in__r, etc.)
  - Average values for Upstream, Internal, Downstream
  - Overall SCRES score
- ‚úÖ Values in debug table match CSV data exactly

### What This Proves:
- Debug mode parameter is passed correctly
- Conditional rendering works
- Correct person's data is being used
- No impact on report when debug mode is off

---

## Test 3: Demo Mode Feature

**Purpose:** Verify demo mode generates report with synthetic data

### Steps:
1. In GUI, still on **Reports** tab
2. Company: **Any company** (doesn't matter)
3. Person: **Any person** (doesn't matter)
4. ‚ùå **UNCHECK** Debug Mode
5. ‚úÖ **CHECK** "Demo Mode (use synthetic test data)"
6. Click **Generate Single Report**

### Expected Results:
- ‚úÖ Report generates successfully
- ‚úÖ Report shows synthetic/random data (NOT real CSV data)
- ‚úÖ All charts render properly
- ‚úÖ No errors or crashes
- ‚úÖ Values are reasonable (between 0-5)

### What This Proves:
- Demo mode parameter is passed correctly
- Report can generate without depending on CSV data
- Useful for testing template changes without real data

---

## Test 4: Both Modes Together

**Purpose:** Verify debug and demo modes work together

### Steps:
1. In GUI, **Reports** tab
2. Company: **Any**
3. Person: **Any**
4. ‚úÖ **CHECK** "Debug Mode"
5. ‚úÖ **CHECK** "Demo Mode"
6. Click **Generate Single Report**

### Expected Results:
- ‚úÖ Report generates with synthetic data
- ‚úÖ Debug table appears at end showing the synthetic values used
- ‚úÖ No errors

### What This Proves:
- Both parameters can be active simultaneously
- Debug table shows synthetic data when demo mode is on

---

## Test 5: Quality Dashboard Button

**Purpose:** Verify quality dashboard button generates analysis

### Steps:
1. Go to **Data** tab
2. Verify data is loaded (467 records)
3. Click **üîç Run Quality Dashboard** button
4. Wait for processing

### Expected Results:
- ‚úÖ Output text box fills with quality analysis:
  - Missing values analysis
  - Value distribution analysis
  - Out of range values check
  - Completion rate analysis
  - Overall quality score (0-100)
- ‚úÖ Popup appears showing PNG file location
- ‚úÖ PNG file created in `data/quality_reports/`
- ‚úÖ PNG shows 4-panel dashboard:
  - Missing values chart
  - Score distribution histogram
  - Completion rate distribution
  - Score distribution by pillar (boxplots)

### What This Proves:
- Quality dashboard button works
- Script runs in background thread
- Visual dashboard generated correctly
- Data quality monitoring is functional

---

## Test 6: Data Cleaner Button

**Purpose:** Verify data cleaner button processes data correctly

### Steps:

**Setup - Create test data with invalid values:**
1. Open `data/cleaned_master.csv` in Excel
2. Find Abbott - Lisa Daly row
3. Change `up__r` value to `?`
4. Change `up__c` value to `N/A`
5. Save file

**Run cleaner:**
6. In GUI, **Data** tab
7. Click **üßπ Run Data Cleaner** button
8. Wait for processing

### Expected Results:
- ‚úÖ Backup created in `data/backups/` with timestamp
- ‚úÖ Output shows cleaning report:
  - "up__r: 1 invalid value(s) (e.g., '?')"
  - "up__c: 1 invalid value(s) (e.g., 'N/A')"
  - "Total invalid values replaced: 2"
- ‚úÖ Alert popup appears mentioning replacements
- ‚úÖ File created: `data/value_replacements_log.csv`
- ‚úÖ Log shows:
  - Row number
  - Company: Abbott
  - Person: Lisa Daly
  - Column: up__r, up__c
  - Original values: ?, N/A
  - Action: set_to_NaN_then_2.5
- ‚úÖ Data auto-reloads in GUI
- ‚úÖ Opening `data/cleaned_master.csv` shows values now 2.5

**Cleanup:**
9. Restore original CSV from backup or reload from source

### What This Proves:
- Data cleaner detects invalid values
- Replacements are logged with full details
- Backups are created before changes
- Auto-reload works after cleaning

---

## Test 7: Batch Generation with Person Parameter

**Purpose:** Verify batch generation uses individual person data

### Steps:
1. Go to **Reports** tab
2. ‚ùå **UNCHECK** both Debug Mode and Demo Mode
3. Click **Start All Reports** (batch generation)
4. Wait for completion

### Expected Results:
- ‚úÖ All reports generate successfully
- ‚úÖ Each person gets their own unique report
- ‚úÖ Check 2-3 reports from same company (e.g., Abbott):
  - Abbott - Lisa Daly: Shows Lisa's unique scores
  - Abbott - Harold Rietveld: Shows Harold's unique scores
  - Scores are DIFFERENT between these two reports
- ‚úÖ No errors in generation log

### Validation Check:
Compare 2 reports from Abbott:
- Lisa Daly vs Harold Rietveld
- Their Upstream/Internal/Downstream averages should be different
- This proves person parameter is working

---

## Test 8: Email Priority Fallback

**Purpose:** Verify email sending uses correct account priority

### Steps:
1. Generate a report for any person
2. Go to **Send Email** tab
3. Select the generated report
4. Enter recipient: **your.test@email.com**
5. Click **Send Selected Emails**
6. **READ THE LOG OUTPUT CAREFULLY**

### Expected Results:
Log should show one of these:
- ‚úÖ `[OK] Using priority account: info@resiliencescan.org` (ideal)
- ‚úÖ `[OK] Using priority account: r.deboer@windesheim.nl` (fallback 1)
- ‚úÖ `[OK] Using priority account: cg.verhoef@windesheim.nl` (fallback 2)
- ‚úÖ `[INFO] No priority account available, using: [some.other@account.com]` (fallback 3)
- ‚úÖ `[INFO] Outlook not available, using SMTP...` (final fallback)

### What This Proves:
- Email priority system works
- System tries accounts in correct order
- Fallback logic is functional
- User is informed which method was used

---

## Test 9: Invalid Data Handling in Report

**Purpose:** Verify report doesn't crash with bad data

### Steps:

**Setup - Create test data:**
1. Open `data/cleaned_master.csv`
2. Find Abbott - Lisa Daly
3. Change several scores to: `?`, `N/A`, `3,5` (European comma), blank
4. Save file

**Generate report WITHOUT cleaning:**
5. In GUI, **Reports** tab
6. Select Abbott - Lisa Daly
7. ‚úÖ **CHECK** Debug Mode (to see what values are used)
8. Click **Generate Single Report**

### Expected Results:
- ‚úÖ Report generates successfully (NO CRASH!)
- ‚úÖ Invalid values replaced with 2.5 automatically
- ‚úÖ European comma `3,5` converted to `3.5`
- ‚úÖ Debug table shows final cleaned values (2.5 where invalid, 3.5 for comma)
- ‚úÖ Report looks normal, just uses 2.5 for missing data

**Cleanup:**
9. Run data cleaner or restore from backup

### What This Proves:
- Robust data cleaning in ResilienceReport.qmd works
- Report never crashes from bad data
- Bad values replaced with neutral midpoint (2.5)
- European number formats handled

---

## Test 10: Validation Script

**Purpose:** Verify validation script still detects errors correctly

### Steps:
1. Generate report for Abbott - Lisa Daly (normal mode, no debug/demo)
2. Open command prompt in project folder
3. Run: `python validate_single_report.py "reports\[report_filename].pdf" Abbott "Lisa Daly"`

### Expected Results:
- ‚úÖ Script extracts scores from PDF
- ‚úÖ Compares against CSV data
- ‚úÖ Shows validation result: PASS or FAIL with details
- ‚úÖ If scores match: "Validation PASSED"
- ‚úÖ If scores differ: Shows expected vs actual values

### What This Proves:
- Validation still works after all changes
- Report data accuracy can be verified
- Person parameter fix is working (each person has unique data)

---

## Summary Checklist

After completing all tests, verify:

- [ ] Test 1: Existing functionality works (baseline)
- [ ] Test 2: Debug mode adds table at end
- [ ] Test 3: Demo mode uses synthetic data
- [ ] Test 4: Both modes work together
- [ ] Test 5: Quality dashboard button generates analysis
- [ ] Test 6: Data cleaner button processes invalid data
- [ ] Test 7: Batch generation gives each person unique data
- [ ] Test 8: Email priority fallback works
- [ ] Test 9: Report handles invalid data gracefully
- [ ] Test 10: Validation script still works

---

## Troubleshooting

### If a test fails:

1. **Check logs:**
   - `gui_log.txt` - GUI errors
   - `data/cleaning_report.txt` - Data cleaner output
   - Console output when running scripts

2. **Check generated files:**
   - `data/value_replacements_log.csv` - Data corrections
   - `data/quality_reports/*.png` - Quality dashboards
   - `test_reports/validation_report_*.txt` - Automated test results

3. **Common issues:**
   - **Report won't generate:** Check Quarto is installed, check data loaded
   - **Debug table not showing:** Verify checkbox was checked before generation
   - **Wrong person data:** Check person parameter is being passed (see Test 7)
   - **Email fails:** Check Outlook is running, check account exists

---

## Automated Tests Already Passed

‚úÖ Data file exists (467 records)
‚úÖ Quality dashboard script runs
‚úÖ Data cleaner script runs
‚úÖ Debug mode parameter configured
‚úÖ Demo mode parameter configured
‚úÖ Person parameter configured
‚úÖ Robust data cleaning implemented
‚úÖ GUI checkboxes configured
‚úÖ GUI quality buttons configured
‚úÖ GUI passes all parameters to Quarto
‚úÖ Generate_all_reports passes person parameter
‚úÖ Email priority fallback configured

**All 12 automated tests passed** - Now complete manual tests above!

---

## Next Steps After Testing

1. If all tests pass ‚Üí System is ready for production use
2. If any test fails ‚Üí Review error logs and fix issues
3. Document any additional issues found
4. Train users on new features (debug mode, quality dashboard, data cleaner)

---

**Testing Date:** ____________
**Tester Name:** ____________
**Overall Status:** [ ] PASS  [ ] FAIL
**Notes:** ________________________________
